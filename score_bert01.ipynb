{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from konlpy.tag import Mecab\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoBERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, topics, rubrics, responses, scores, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mecab = Mecab()\n",
    "        self.data = self._preprocess_data(topics, rubrics, responses, scores)\n",
    "\n",
    "    def _preprocess_data(self, topics, rubrics, responses, scores):\n",
    "        data = []\n",
    "        for topic, rubric, response, score in zip(topics, rubrics, responses, scores):\n",
    "            text = self._combine_text(topic, rubric, response)\n",
    "            text = self._preprocess_text(text)\n",
    "            inputs = self.tokenizer(text, max_length=self.max_length, truncation=True)\n",
    "            data.append((inputs[\"input_ids\"], inputs[\"attention_mask\"], score))\n",
    "        return data\n",
    "\n",
    "    def _combine_text(self, topic, rubrics, response):\n",
    "        text = f\"**주제:** {topic}\\n\\n\"\n",
    "        text += f\"**평가항목:** {rubrics} ({response})\\n\\n\"\n",
    "        return text\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        stopwords = ['은', '는', '이', '가', '의', '을', '를', '으로', '에서', '에', '한', '하는']\n",
    "        text = ' '.join(word for word in self.mecab.morphs(text) if word not in stopwords)\n",
    "        text = re.sub(r'\\d+', '0', text)  # Normalize numbers\n",
    "        text = re.sub(r'[a-zA-Z]+', 'a', text)  # Normalize English letters\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove non-alphanumeric characters\n",
    "        return text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from konlpy.tag import Mecab\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class KoBERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, topics, rubrics, responses, scores, rubric_max_scores, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mecab = Mecab()\n",
    "        self.rubric_max_scores = rubric_max_scores\n",
    "        self.data = self._preprocess_data(topics, rubrics, responses, scores)\n",
    "\n",
    "    def _preprocess_data(self, topics, rubrics, responses, scores):\n",
    "        data = []\n",
    "        for topic, rubric, response, score in zip(topics, rubrics, responses, scores):\n",
    "            text = self._combine_text(topic, rubric, response)\n",
    "            text = self._preprocess_text(text)\n",
    "            inputs = self.tokenizer(text, max_length=self.max_length, truncation=True)\n",
    "            normalized_scores = [s / m for s, m in zip(score, self.rubric_max_scores)]\n",
    "            data.append((inputs[\"input_ids\"], inputs[\"attention_mask\"], normalized_scores))\n",
    "        return data\n",
    "\n",
    "    def _combine_text(self, topic, rubrics, response):\n",
    "        text = f\"**주제:** {topic}\\n\\n\"\n",
    "        for r, s in zip(rubrics, response):\n",
    "            text += f\"**평가항목:** {r} ({s})\\n\\n\"\n",
    "        return text\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        stopwords = ['은', '는', '이', '가', '의', '을', '를', '으로', '에서', '에', '한', '하는']\n",
    "        text = ' '.join(word for word in self.mecab.morphs(text) if word not in stopwords)\n",
    "        text = re.sub(r'\\d+', '0', text)  # Normalize numbers\n",
    "        text = re.sub(r'[a-zA-Z]+', 'a', text)  # Normalize English letters\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove non-alphanumeric characters\n",
    "        return text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoBertRegressorModel(nn.Module):\n",
    "    def __init__(self, num_labels, config=None):\n",
    "        super().__init__()\n",
    "        if config is None:\n",
    "            config = BertConfig.from_pretrained('monologg/kobert')\n",
    "        self.bert = BertModel.from_pretrained('monologg/kobert', config=config)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.regressor = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.regressor(pooled_output)\n",
    "        return logits\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, optimizer, criterion, num_epochs, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            input_ids, attention_mask, labels = [data.to(device) for data in batch]\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for batch in val_dataloader:\n",
    "                input_ids, attention_mask, labels = [data.to(device) for data in batch]\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "            val_loss /= len(val_dataloader)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "def evaluate_model(model, dataloader, device, num_rubrics):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    mse_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask, labels = [data.to(device) for data in batch]\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            all_predictions.extend(predictions.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "            mse_loss += nn.MSELoss()(outputs, labels)\n",
    "\n",
    "    accuracy = sum(p == l for p, l in zip(all_predictions, all_labels)) / len(all_labels)\n",
    "    rubric_scores = []\n",
    "    for rubric_idx in range(num_rubrics):\n",
    "        rubric_predictions = [p[rubric_idx] for p in all_predictions]\n",
    "        rubric_labels = [l[rubric_idx] for l in all_labels]\n",
    "        rubric_accuracy = sum(p == l for p, l in zip(rubric_predictions, rubric_labels)) / len(rubric_labels)\n",
    "        rubric_scores.append(rubric_accuracy)\n",
    "    mse_loss /= len(dataloader)\n",
    "\n",
    "    return accuracy, rubric_scores, mse_loss\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, sep='\\t')\n",
    "    topics = data['topic'].tolist()\n",
    "    rubrics = [data[f'rubric{i}'].tolist() for i in range(1, 4)]\n",
    "    rubrics = list(zip(*rubrics))\n",
    "    responses = [data[f'response{i}'].tolist() for i in range(1, 4)]\n",
    "    responses = list(zip(*responses))\n",
    "    scores = [data[f'score{i}'].tolist() for i in range(1, 4)]\n",
    "    scores = list(zip(*scores))\n",
    "    rubric_max_scores = [data[f'rubric{i}_max_score'].tolist()[0] for i in range(1, 4)]\n",
    "    return topics, rubrics, responses, scores, rubric_max_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'scoring_data.xlsx'  # CSV 파일 경로\n",
    "topics, rubrics, responses, scores, rubric_max_scores = load_data(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # topics = [...]  # 주제글 데이터\n",
    "    # rubrics = [...]  # 루브릭 데이터\n",
    "    # responses = [...]  # 응답글 데이터\n",
    "    # scores = [...]  # 점수 데이터\n",
    "    data_file = 'data.tsv'  # CSV 파일 경로\n",
    "    topics, rubrics, responses, scores, rubric_max_scores = load_data(data_file)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    batch_size = 16\n",
    "    learning_rate = 2e-5\n",
    "    max_length = 512\n",
    "    num_labels = 3\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "    dataset = KoBERTDataset(topics, rubrics, responses, scores, rubric_max_scores, tokenizer, max_length=512)\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = KoBertRegressorModel(num_labels)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_model(model, train_dataloader, val_dataloader, optimizer, criterion, num_epochs, device)\n",
    "\n",
    "    test_topics = [...]  # 테스트 주제글 데이터\n",
    "    test_rubrics = [...]  # 테스트 루브릭 데이터\n",
    "    test_responses = [...]  # 테스트 응답글 데이터\n",
    "    test_scores = [...]  # 테스트 점수 데이터\n",
    "\n",
    "    test_dataset = KoBERTDataset(test_topics, test_rubrics, test_responses, test_scores, tokenizer, max_length)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    accuracy, rubric_scores, mse_loss = evaluate_model(model, test_dataloader, device, num_labels)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Rubric Scores: {rubric_scores}\")\n",
    "    print(f\"MSE Loss: {mse_loss:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
