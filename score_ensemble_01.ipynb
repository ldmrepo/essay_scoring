{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 모델 클래스:\n",
    "   - `KoBERTRubricScorer`: 루브릭 기반 점수 예측을 위한 KoBERT 회귀 모델\n",
    "   - `KoBERTRubricClassifier`: 루브릭 기반 점수 분류를 위한 KoBERT 분류 모델\n",
    "   - `KoBERTRubricMTLScorer`: 루브릭 기반 점수 예측을 위한 KoBERT MTL 모델\n",
    "   - `KoBERTRubricMTLRegressor`: 루브릭 기반 점수 예측을 위한 KoBERT MTL 회귀 모델\n",
    "\n",
    "2. 데이터셋 클래스:\n",
    "   - `RubricScoringDataset`: 루브릭 기반 점수 평가 데이터셋을 로드하고 전처리하는 클래스\n",
    "\n",
    "3. 데이터 로더 함수:\n",
    "   - `create_data_loader`: 데이터셋을 배치 단위로 로드하는 데이터 로더를 생성하는 함수\n",
    "\n",
    "4. 트레이너 클래스:\n",
    "   - `RubricScorerTrainer`: 모델 학습을 수행하는 트레이너 클래스\n",
    "\n",
    "5. 앙상블 클래스:\n",
    "   - `RubricScorerEnsemble`: 여러 모델의 예측 결과를 앙상블하는 클래스\n",
    "\n",
    "6. 평가 함수:\n",
    "   - `evaluate_regression`: 회귀 모델의 평가 지표(MSE)를 계산하는 함수\n",
    "   - `evaluate_classification`: 분류 모델의 평가 지표(정확도, F1 점수)를 계산하는 함수\n",
    "\n",
    "7. 데이터 전처리 및 증강 함수:\n",
    "   - `preprocess_text`: 텍스트 데이터를 전처리하는 함수\n",
    "   - `augment_text`: 텍스트 데이터를 증강하는 함수\n",
    "\n",
    "8. 메인 함수:\n",
    "   - 데이터 로드, 전처리, 모델 생성, 학습, 앙상블, 평가 등의 전체 프로세스를 수행합니다.\n",
    "\n",
    "코드의 실행 흐름은 다음과 같습니다:\n",
    "\n",
    "1. 데이터 로드 및 전처리\n",
    "2. 모델 생성 및 옵티마이저, 손실 함수 설정\n",
    "3. 각 모델 별로 `RubricScorerTrainer`를 사용하여 학습 수행\n",
    "4. 앙상블 모델 생성 및 예측\n",
    "5. 회귀 모델과 분류 모델의 평가 지표 계산 및 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertConfig, BertTokenizer\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from konlpy.tag import Mecab, Okt\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# 모델\n",
    "class KoBERTRubricScorer(nn.Module):\n",
    "    def __init__(self, num_tasks, hidden_size=768, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        config = BertConfig.from_pretrained('monologg/kobert')\n",
    "        config.num_labels = num_tasks\n",
    "        self.bert = BertModel.from_pretrained('monologg/kobert', config=config)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.regressor = nn.Linear(hidden_size, num_tasks)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.regressor(pooled_output)\n",
    "        return logits\n",
    "\n",
    "class KoBERTRubricClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size=768, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        config = BertConfig.from_pretrained('monologg/kobert')\n",
    "        config.num_labels = num_classes\n",
    "        self.bert = BertModel.from_pretrained('monologg/kobert', config=config)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "class KoBERTRubricMTLScorer(nn.Module):\n",
    "    def __init__(self, num_tasks, hidden_size=768, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        config = BertConfig.from_pretrained('monologg/kobert')\n",
    "        self.bert = BertModel.from_pretrained('monologg/kobert', config=config)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.regression_heads = nn.ModuleList([nn.Linear(hidden_size, 1) for _ in range(num_tasks)])\n",
    "        self.classification_head = nn.Linear(hidden_size, 5)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        regression_outputs = [head(pooled_output) for head in self.regression_heads]\n",
    "        regression_outputs = torch.cat(regression_outputs, dim=-1)\n",
    "        classification_output = self.classification_head(pooled_output)\n",
    "        return regression_outputs, classification_output\n",
    "\n",
    "class KoBERTRubricMTLRegressor(nn.Module):\n",
    "    def __init__(self, num_tasks, hidden_size=768, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        config = BertConfig.from_pretrained('monologg/kobert')\n",
    "        self.bert = BertModel.from_pretrained('monologg/kobert', config=config)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.regression_heads = nn.ModuleList([nn.Linear(hidden_size, 1) for _ in range(num_tasks)])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        regression_outputs = [head(pooled_output) for head in self.regression_heads]\n",
    "        regression_outputs = torch.cat(regression_outputs, dim=-1)\n",
    "        return regression_outputs\n",
    "\n",
    "# 데이터셋\n",
    "class RubricScoringDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length, okt=None, stopwords=None, synonym_dict=None, augment_ratio=0.0):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.okt = okt\n",
    "        self.stopwords = stopwords\n",
    "        self.synonym_dict = synonym_dict\n",
    "        self.augment_ratio = augment_ratio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        text = self.preprocess_text(item['text'])\n",
    "        if self.augment_ratio > 0 and random.random() < self.augment_ratio:\n",
    "            text = self.augment_text(text)\n",
    "        inputs = self.tokenizer(text, max_length=self.max_length, truncation=True, padding='max_length',\n",
    "                                return_tensors='pt')\n",
    "        labels = torch.tensor(item['labels'], dtype=torch.float32)\n",
    "        return inputs['input_ids'].squeeze(), inputs['attention_mask'].squeeze(), labels\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        if self.okt is not None and self.stopwords is not None:\n",
    "            tokens = self.okt.morphs(text)\n",
    "            tokens = [token for token in tokens if token not in self.stopwords]\n",
    "            text = ' '.join(tokens)\n",
    "        return text\n",
    "\n",
    "    def augment_text(self, text):\n",
    "        if self.synonym_dict is not None:\n",
    "            words = text.split()\n",
    "            for i, word in enumerate(words):\n",
    "                if word in self.synonym_dict and random.random() < 0.1:\n",
    "                    words[i] = random.choice(self.synonym_dict[word])\n",
    "            text = ' '.join(words)\n",
    "        return text\n",
    "\n",
    "# 데이터 로더\n",
    "def create_data_loader(dataset, batch_size, shuffle=True):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# 트레이너\n",
    "class RubricScorerTrainer:\n",
    "    def __init__(self, model, train_dataloader, val_dataloader, optimizer, criterion, device):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "\n",
    "            for batch in self.train_dataloader:\n",
    "                input_ids, attention_mask, labels = [data.to(self.device) for data in batch]\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "\n",
    "                if isinstance(self.model, KoBERTRubricClassifier):\n",
    "                    loss = self.criterion(outputs, labels[:, 3].long())\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    train_correct += (predicted == labels[:, 3].long()).sum().item()\n",
    "                    train_total += labels.size(0)\n",
    "                else:\n",
    "                    loss = self.criterion(outputs, labels[:, :3])\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            train_loss /= len(self.train_dataloader)\n",
    "            train_accuracy = train_correct / train_total if isinstance(self.model, KoBERTRubricClassifier) else None\n",
    "\n",
    "            val_loss, val_accuracy = self.validate()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy if train_accuracy else '-'}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy if val_accuracy else '-'}\")\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_dataloader:\n",
    "                input_ids, attention_mask, labels = [data.to(self.device) for data in batch]\n",
    "\n",
    "                outputs = self.model(input_ids, attention_mask)\n",
    "\n",
    "                if isinstance(self.model, KoBERTRubricClassifier):\n",
    "                    loss = self.criterion(outputs, labels[:, 3].long())\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_correct += (predicted == labels[:, 3].long()).sum().item()\n",
    "                    val_total += labels.size(0)\n",
    "                else:\n",
    "                    loss = self.criterion(outputs, labels[:, :3])\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(self.val_dataloader)\n",
    "        val_accuracy = val_correct / val_total if isinstance(self.model, KoBERTRubricClassifier) else None\n",
    "\n",
    "        return val_loss, val_accuracy\n",
    "\n",
    "# 앙상블\n",
    "class RubricScorerEnsemble:\n",
    "    def __init__(self, models, ensemble_method='mean'):\n",
    "        self.models = models\n",
    "        self.ensemble_method = ensemble_method\n",
    "\n",
    "    def predict(self, dataloader, device):\n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            model_predictions = []\n",
    "            with torch.no_grad():\n",
    "                for batch in dataloader:\n",
    "                    input_ids, attention_mask, _ = [data.to(device) for data in batch]\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    if isinstance(model, KoBERTRubricClassifier):\n",
    "                        outputs = torch.softmax(outputs, dim=1)\n",
    "                    model_predictions.extend(outputs.cpu().numpy())\n",
    "            predictions.append(model_predictions)\n",
    "\n",
    "        if self.ensemble_method == 'mean':\n",
    "            ensemble_predictions = np.mean(predictions, axis=0)\n",
    "        elif self.ensemble_method == 'weighted_mean':\n",
    "            weights = self._calculate_weights(predictions)\n",
    "            ensemble_predictions = np.average(predictions, axis=0, weights=weights)\n",
    "        elif self.ensemble_method == 'voting':\n",
    "            ensemble_predictions = self._voting_ensemble(predictions)\n",
    "        else:\n",
    "            raise ValueError(f\"지원하지 않는 앙상블 방법입니다: {self.ensemble_method}\")\n",
    "\n",
    "        return ensemble_predictions\n",
    "\n",
    "    def _calculate_weights(self, predictions):\n",
    "        # 각 모델의 예측 결과를 기반으로 가중치를 계산하는 로직을 구현합니다.\n",
    "        # 예시: 각 모델의 예측 결과의 표준편차의 역수로 가중치를 설정\n",
    "        weights = [1 / np.std(pred) for pred in predictions]\n",
    "        weights = np.array(weights) / np.sum(weights)\n",
    "        return weights\n",
    "\n",
    "    def _voting_ensemble(self, predictions):\n",
    "        # 투표 기반 앙상블을 수행합니다.\n",
    "        voted_predictions = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=np.argmax(predictions, axis=2))\n",
    "        return voted_predictions\n",
    "\n",
    "# 평가\n",
    "def evaluate_regression(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return mse\n",
    "\n",
    "def evaluate_classification(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    return accuracy, f1\n",
    "\n",
    "# 데이터 전처리 및 증강\n",
    "def preprocess_text(text, okt, stopwords):\n",
    "    # 형태소 분석 및 불용어 제거 등의 전처리 수행\n",
    "    tokens = okt.morphs(text)\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def augment_text(text, okt, synonym_dict, augment_ratio):\n",
    "    # 동의어 대체, 삽입, 삭제 등의 데이터 증강 수행\n",
    "    if random.random() < augment_ratio:\n",
    "        # 동의어 대체\n",
    "        words = text.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word in synonym_dict and random.random() < 0.1:\n",
    "                words[i] = random.choice(synonym_dict[word])\n",
    "        text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메인 함수\n",
    "def main():\n",
    "    # 데이터 로드\n",
    "    train_data = pd.read_csv('train_data.csv')\n",
    "    val_data = pd.read_csv('val_data.csv')\n",
    "    test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "    # 토크나이저 및 전처리 도구 로드\n",
    "    tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "    okt = Okt()\n",
    "    stopwords = ['은', '는', '이', '가', '을', '를']\n",
    "    synonym_dict = {'좋아하다': ['즐기다', '기뻐하다'], '훌륭하다': ['우수하다', '뛰어나다']}\n",
    "\n",
    "    # 데이터셋 생성\n",
    "    train_dataset = RubricScoringDataset(train_data, tokenizer, max_length=128, okt=okt, synonym_dict=synonym_dict, augment_ratio=0.1)\n",
    "    val_dataset = RubricScoringDataset(val_data, tokenizer, max_length=128)\n",
    "    test_dataset = RubricScoringDataset(test_data, tokenizer, max_length=128)\n",
    "\n",
    "    # 데이터 로더 생성\n",
    "    train_dataloader = create_data_loader(train_dataset, batch_size=16)\n",
    "    val_dataloader = create_data_loader(val_dataset, batch_size=16)\n",
    "    test_dataloader = create_data_loader(test_dataset, batch_size=16)\n",
    "\n",
    "    # 모델 생성\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    rubric_scorer = KoBERTRubricScorer(num_tasks=3).to(device)\n",
    "    rubric_classifier = KoBERTRubricClassifier(num_classes=5).to(device)\n",
    "    mtl_rubric_scorer = KoBERTRubricMTLScorer(num_tasks=3).to(device)\n",
    "    mtl_rubric_regressor = KoBERTRubricMTLRegressor(num_tasks=3).to(device)\n",
    "\n",
    "    # 옵티마이저 및 손실 함수 설정\n",
    "    scorer_optimizer = torch.optim.Adam(rubric_scorer.parameters(), lr=1e-5)\n",
    "    scorer_criterion = nn.MSELoss()\n",
    "\n",
    "    classifier_optimizer = torch.optim.Adam(rubric_classifier.parameters(), lr=1e-5)\n",
    "    classifier_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    mtl_scorer_optimizer = torch.optim.Adam(mtl_rubric_scorer.parameters(), lr=1e-5)\n",
    "    mtl_scorer_criterion = nn.MSELoss()\n",
    "\n",
    "    mtl_regressor_optimizer = torch.optim.Adam(mtl_rubric_regressor.parameters(), lr=1e-5)\n",
    "    mtl_regressor_criterion = nn.MSELoss()\n",
    "    # 훈련\n",
    "    num_epochs = 10\n",
    "\n",
    "    # Rubric Scorer 훈련\n",
    "    print(\"Training Rubric Scorer...\")\n",
    "    rubric_scorer_trainer = RubricScorerTrainer(rubric_scorer, train_dataloader, val_dataloader,\n",
    "                                                scorer_optimizer, scorer_criterion, device)\n",
    "    rubric_scorer_trainer.train(num_epochs)\n",
    "\n",
    "    # Rubric Classifier 훈련\n",
    "    print(\"Training Rubric Classifier...\")\n",
    "    rubric_classifier_trainer = RubricScorerTrainer(rubric_classifier, train_dataloader, val_dataloader,\n",
    "                                                    classifier_optimizer, classifier_criterion, device)\n",
    "    rubric_classifier_trainer.train(num_epochs)\n",
    "\n",
    "    # MTL Rubric Scorer 훈련\n",
    "    print(\"Training MTL Rubric Scorer...\")\n",
    "    mtl_rubric_scorer_trainer = RubricScorerTrainer(mtl_rubric_scorer, train_dataloader, val_dataloader,\n",
    "                                                    mtl_scorer_optimizer, mtl_scorer_criterion, device)\n",
    "    mtl_rubric_scorer_trainer.train(num_epochs)\n",
    "\n",
    "    # MTL Rubric Regressor 훈련\n",
    "    print(\"Training MTL Rubric Regressor...\")\n",
    "    mtl_rubric_regressor_trainer = RubricScorerTrainer(mtl_rubric_regressor, train_dataloader, val_dataloader,\n",
    "                                                    mtl_regressor_optimizer, mtl_regressor_criterion, device)\n",
    "    mtl_rubric_regressor_trainer.train(num_epochs)\n",
    "\n",
    "    # 앙상블 설정\n",
    "    ensemble_method = 'weighted_mean'\n",
    "    models = [rubric_scorer, rubric_classifier, mtl_rubric_scorer, mtl_rubric_regressor]\n",
    "    ensemble = RubricScorerEnsemble(models, ensemble_method)\n",
    "\n",
    "    # 평가\n",
    "    ensemble_predictions = ensemble.predict(test_dataloader, device)\n",
    "\n",
    "    # 회귀 모델 평가\n",
    "    regression_labels = test_data['labels'].values[:, :3]\n",
    "    regression_mse = evaluate_regression(regression_labels, ensemble_predictions[:, :3])\n",
    "    print(f\"Ensemble Regression MSE: {regression_mse:.4f}\")\n",
    "\n",
    "    # 분류 모델 평가\n",
    "    classification_labels = test_data['labels'].values[:, 3]\n",
    "    classification_predictions = np.argmax(ensemble_predictions[:, 3:], axis=1)\n",
    "    accuracy, f1 = evaluate_classification(classification_labels, classification_predictions)\n",
    "    print(f\"Ensemble Classification Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Ensemble Classification F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
