{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torchviz import make_dot\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "\n",
    "    def remove_stop_words(self, text):\n",
    "        return ' '.join([word for word in text.split() if word.lower() not in self.stop_words])\n",
    "\n",
    "class DataAugmenter:\n",
    "    def __init__(self, synonym_dict, insertion_words, deletion_prob):\n",
    "        self.synonym_dict = synonym_dict\n",
    "        self.insertion_words = insertion_words\n",
    "        self.deletion_prob = deletion_prob\n",
    "\n",
    "    def synonym_replacement(self, text):\n",
    "        words = text.split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word in self.synonym_dict:\n",
    "                words[i] = random.choice(self.synonym_dict[word])\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def random_insertion(self, text):\n",
    "        words = text.split()\n",
    "        if len(words) < 2:\n",
    "            return text\n",
    "        insertion_idx = random.randint(0, len(words) - 1)\n",
    "        words.insert(insertion_idx, random.choice(self.insertion_words))\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def random_deletion(self, text):\n",
    "        words = text.split()\n",
    "        if len(words) < 2:\n",
    "            return text\n",
    "        deletion_indices = [i for i, _ in enumerate(words) if random.random() < self.deletion_prob]\n",
    "        for i in sorted(deletion_indices, reverse=True):\n",
    "            del words[i]\n",
    "        return ' '.join(words)\n",
    "\n",
    "class ScoringDataset:\n",
    "    def __init__(self, file_path, preprocessor, augmenter):\n",
    "        self.data = pd.read_excel(file_path)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.max_len = 512\n",
    "        self.preprocessor = preprocessor\n",
    "        self.augmenter = augmenter\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "    def preprocess(self):\n",
    "        self.data['text'] = self.data['주제글'] + ' ' + self.data['모범글'] + ' ' + self.data['수험자 응답글']\n",
    "        self.data['text'] = self.data['text'].apply(self.preprocessor.remove_stop_words)\n",
    "        self.X = self.data['text'].values\n",
    "        self.y = self.data[['루블릭1점수', '루블릭2점수', '루블릭3점수']].values\n",
    "\n",
    "    def augment(self):\n",
    "        augmented_data = []\n",
    "        for text in self.X:\n",
    "            augmented_text = self.augmenter.synonym_replacement(text)\n",
    "            augmented_text = self.augmenter.random_insertion(augmented_text)\n",
    "            augmented_text = self.augmenter.random_deletion(augmented_text)\n",
    "            augmented_data.append(augmented_text)\n",
    "        self.X = np.concatenate((self.X, np.array(augmented_data)))\n",
    "        self.y = np.concatenate((self.y, self.y))\n",
    "\n",
    "    def tokenize(self):\n",
    "        X_tokenized = self.tokenizer.batch_encode_plus(\n",
    "            self.X.tolist(),\n",
    "            max_length=self.max_len,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        self.X = X_tokenized['input_ids']\n",
    "\n",
    "    def split_data(self, test_size=0.2, val_size=0.2):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=test_size, random_state=42)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=42)\n",
    "        return X_train, X_val, X_test, torch.tensor(y_train, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(self.hidden_dim)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        context_vector = torch.matmul(attention_weights, value)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class ScoringModel(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(ScoringModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.attention1 = AttentionLayer(self.bert.config.hidden_size)\n",
    "        self.attention2 = AttentionLayer(self.bert.config.hidden_size)\n",
    "        self.attention3 = AttentionLayer(self.bert.config.hidden_size)\n",
    "        self.hidden = nn.Linear(self.bert.config.hidden_size, hidden_dim)\n",
    "        self.output1 = nn.Linear(hidden_dim, 1)\n",
    "        self.output2 = nn.Linear(hidden_dim, 1)\n",
    "        self.output3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.size(1) > 512:\n",
    "            x = x[:, :512]  # 입력 데이터의 길이를 512로 조정\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)  # 배치 차원 추가\n",
    "        outputs = self.bert(x)\n",
    "        last_hidden_state = outputs[0]\n",
    "        context_vector1, attention_weights1 = self.attention1(last_hidden_state)\n",
    "        context_vector2, attention_weights2 = self.attention2(last_hidden_state)\n",
    "        context_vector3, attention_weights3 = self.attention3(last_hidden_state)\n",
    "        hidden1 = torch.relu(self.hidden(context_vector1[:, 0, :]))\n",
    "        hidden2 = torch.relu(self.hidden(context_vector2[:, 0, :]))\n",
    "        hidden3 = torch.relu(self.hidden(context_vector3[:, 0, :]))\n",
    "        output1 = self.output1(hidden1)\n",
    "        output2 = self.output2(hidden2)\n",
    "        output3 = self.output3(hidden3)\n",
    "        return output1, output2, output3, attention_weights1, attention_weights2, attention_weights3\n",
    "    \n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, X_train, y_train, X_val, y_val, epochs=5, batch_size=16):\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=2e-5)\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(0, len(self.X_train), self.batch_size):\n",
    "                batch_X = self.X_train[i:i + self.batch_size]\n",
    "                batch_y = self.y_train[i:i + self.batch_size]\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output1, output2, output3, _, _, _ = self.model(batch_X)\n",
    "                loss1 = self.criterion(output1, batch_y[:, 0].unsqueeze(1))\n",
    "                loss2 = self.criterion(output2, batch_y[:, 1].unsqueeze(1))\n",
    "                loss3 = self.criterion(output3, batch_y[:, 2].unsqueeze(1))\n",
    "                loss = loss1 + loss2 + loss3\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                output1, output2, output3, _, _, _ = self.model(self.X_val)\n",
    "                loss1 = self.criterion(output1, self.y_val[:, 0].unsqueeze(1))\n",
    "                loss2 = self.criterion(output2, self.y_val[:, 1].unsqueeze(1))\n",
    "                loss3 = self.criterion(output3, self.y_val[:, 2].unsqueeze(1))\n",
    "                val_loss = loss1 + loss2 + loss3\n",
    "                self.model.train()\n",
    "\n",
    "                self.train_losses.append(loss.item())\n",
    "                self.val_losses.append(val_loss.item())\n",
    "\n",
    "                print(f\"Epoch [{epoch + 1}/{self.epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "    def visualize_training(self):\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(self.train_losses, label='Train Loss')\n",
    "        plt.plot(self.val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.show()\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, model, X_test, y_test, tokenizer):\n",
    "        self.model = model\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test.numpy()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def visualize_performance(self, y_pred):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        for i in range(3):\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            plt.scatter(self.y_test[:, i], y_pred[:, i], alpha=0.5)\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.xlabel(f'True Rubric {i+1} Score')\n",
    "            plt.ylabel(f'Predicted Rubric {i+1} Score')\n",
    "            plt.title(f'Rubric {i+1} Performance')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            output1, output2, output3, attention_weights1, attention_weights2, attention_weights3 = self.model(self.X_test)\n",
    "            y_pred = torch.cat((output1, output2, output3), dim=1).numpy()\n",
    "\n",
    "        mse = mean_squared_error(self.y_test, y_pred)\n",
    "        mae = mean_absolute_error(self.y_test, y_pred)\n",
    "        r2 = r2_score(self.y_test, y_pred)\n",
    "\n",
    "        print(f'Test MSE: {mse:.4f}')\n",
    "        print(f'Test MAE: {mae:.4f}')\n",
    "        print(f'Test R^2: {r2:.4f}')\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        for i in range(3):\n",
    "            plt.subplot(1, 3, i + 1)\n",
    "            plt.scatter(self.y_test[:, i], y_pred[:, i], alpha=0.5)\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.xlabel(f'True Rubric {i + 1} Score')\n",
    "            plt.ylabel(f'Predicted Rubric {i + 1} Score')\n",
    "            plt.title(f'Rubric {i + 1}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        self.visualize_attention(attention_weights1, attention_weights2, attention_weights3)\n",
    "        self.analyze_rubric_attention(attention_weights1, attention_weights2, attention_weights3)\n",
    "\n",
    "        self.visualize_performance(y_pred)\n",
    "\n",
    "        return mse, mae, r2\n",
    "\n",
    "    def visualize_attention(self, attention_weights1, attention_weights2, attention_weights3):\n",
    "        test_sample = self.X_test[0]\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(test_sample.numpy())\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        for i, (ax, attention_weights) in enumerate(zip(axes.flat, [attention_weights1, attention_weights2, attention_weights3]), 1):\n",
    "            attention = attention_weights[0].numpy()\n",
    "            heatmap = ax.imshow(attention, cmap='hot', interpolation='nearest', aspect='auto')\n",
    "            ax.set_xticks(range(len(tokens)))\n",
    "            ax.set_xticklabels(tokens, rotation=45)\n",
    "            ax.set_yticks(range(attention.shape[0]))\n",
    "            ax.set_yticklabels([f'Layer {j+1}' for j in range(attention.shape[0])])\n",
    "            ax.set_title(f'Attention Visualization - Rubric {i}')\n",
    "        fig.colorbar(heatmap, ax=axes.ravel().tolist())\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def analyze_rubric_attention(self, attention_weights1, attention_weights2, attention_weights3):\n",
    "        test_sample = self.X_test[0]\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(test_sample.numpy())\n",
    "\n",
    "        attention_scores1 = attention_weights1[0].mean(dim=0).numpy()\n",
    "        attention_scores2 = attention_weights2[0].mean(dim=0).numpy()\n",
    "        attention_scores3 = attention_weights3[0].mean(dim=0).numpy()\n",
    "\n",
    "        top_indices1 = attention_scores1.argsort()[-10:][::-1]\n",
    "        top_indices2 = attention_scores2.argsort()[-10:][::-1]\n",
    "        top_indices3 = attention_scores3.argsort()[-10:][::-1]\n",
    "\n",
    "        print(\"Rubric 1 - Top Attended Tokens:\")\n",
    "        for idx in top_indices1:\n",
    "            print(f\"{tokens[idx]}: {attention_scores1[idx]:.3f}\")\n",
    "\n",
    "        print(\"\\nRubric 2 - Top Attended Tokens:\")\n",
    "        for idx in top_indices2:\n",
    "            print(f\"{tokens[idx]}: {attention_scores2[idx]:.3f}\")\n",
    "\n",
    "        print(\"\\nRubric 3 - Top Attended Tokens:\")\n",
    "        for idx in top_indices3:\n",
    "            print(f\"{tokens[idx]}: {attention_scores3[idx]:.3f}\")        \n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = 512\n",
    "\n",
    "    def predict_scores(self, prompts, exemplars, responses):\n",
    "        X_new = [prompt + ' ' + exemplar + ' ' + response for prompt, exemplar, response in zip(prompts, exemplars, responses)]\n",
    "\n",
    "        X_new_tokenized = self.tokenizer.batch_encode_plus(\n",
    "            X_new,\n",
    "            max_length=self.max_len,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        X_new_tensor = X_new_tokenized['input_ids']\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            output1, output2, output3, _, _, _ = self.model(X_new_tensor)\n",
    "            y_pred = torch.cat((output1, output2, output3), dim=1).numpy()\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "def main():\n",
    "    file_path = 'scoring_data.xlsx'\n",
    "\n",
    "    stop_words = ['은', '는', '이', '가', '을', '를', '에', '의', '과', '도', '으로', '만', '겠다', '습니다', '니다', '하다']\n",
    "    preprocessor = DataPreprocessor(stop_words)\n",
    "\n",
    "    synonym_dict = {'좋다': ['훌륭하다', '우수하다'], '크다': ['거대하다', '광대하다'], '작다': ['조그맣다', '왜소하다']}\n",
    "    insertion_words = ['그리고', '또한', '게다가']\n",
    "    deletion_prob = 0.2\n",
    "    augmenter = DataAugmenter(synonym_dict, insertion_words, deletion_prob)\n",
    "\n",
    "    dataset = ScoringDataset(file_path, preprocessor, augmenter)\n",
    "    dataset.preprocess()\n",
    "    dataset.augment()\n",
    "    dataset.tokenize()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = dataset.split_data()\n",
    "\n",
    "    hidden_dim = 64\n",
    "\n",
    "    model = ScoringModel(hidden_dim)\n",
    "\n",
    "    # 모델 구조 시각화\n",
    "    input_data = torch.randn(1, dataset.max_len).long()\n",
    "    input_data = input_data[:, :512]  # 입력 데이터의 길이를 512로 조정\n",
    "    output1, output2, output3, _, _, _ = model(input_data)\n",
    "    graph = make_dot(output1, params=dict(model.named_parameters()))\n",
    "    graph.render(\"model_graph\", format=\"png\")\n",
    "\n",
    "    trainer = Trainer(model, X_train, y_train, X_val, y_val, epochs=5, batch_size=16)\n",
    "    trainer.train()\n",
    "    trainer.visualize_training()\n",
    "\n",
    "    evaluator = Evaluator(model, X_test, y_test, dataset.tokenizer)\n",
    "    mse, mae, r2 = evaluator.evaluate()\n",
    "\n",
    "    predictor = Predictor(model, dataset.tokenizer)\n",
    "    prompts = ['새로운 주제글 1', '새로운 주제글 2']\n",
    "    exemplars = ['새로운 모범글 1', '새로운 모범글 2']\n",
    "    responses = ['새로운 응답글 1', '새로운 응답글 2']\n",
    "    y_pred = predictor.predict_scores(prompts, exemplars, responses)\n",
    "\n",
    "    print('\\n예측 결과:')\n",
    "    for i in range(len(prompts)):\n",
    "        print(f'주제글: {prompts[i]}')\n",
    "        print(f'모범글: {exemplars[i]}')\n",
    "        print(f'응답글: {responses[i]}')\n",
    "        print(f'예측 점수 - 루블릭1: {y_pred[i][0]:.2f}, 루블릭2: {y_pred[i][1]:.2f}, 루블릭3: {y_pred[i][2]:.2f}')\n",
    "        print('---')\n",
    "        \n",
    "    print('\\n모델 성능 정리:')\n",
    "    print(f'Test MSE: {mse:.4f}')\n",
    "    print(f'Test MAE: {mae:.4f}')\n",
    "    print(f'Test R^2: {r2:.4f}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네, 전체 코드를 리팩토링하여 모델 구조 시각화, 학습 과정 시각화, 모델 성능 시각화 모듈을 포함하도록 하겠습니다. 아래는 리팩토링된 전체 코드입니다.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torchviz import make_dot\n",
    "\n",
    "class DataPreprocessor:\n",
    "    # ... (생략) ...\n",
    "\n",
    "class DataAugmenter:\n",
    "    # ... (생략) ...\n",
    "\n",
    "class ScoringDataset:\n",
    "    # ... (생략) ...\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    # ... (생략) ...\n",
    "\n",
    "class ScoringModel(nn.Module):\n",
    "    # ... (생략) ...\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, X_train, y_train, X_val, y_val, epochs=5, batch_size=16):\n",
    "        # ... (생략) ...\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def train(self):\n",
    "        # ... (생략) ...\n",
    "        for epoch in range(self.epochs):\n",
    "            # ... (생략) ...\n",
    "            self.train_losses.append(loss.item())\n",
    "            self.val_losses.append(val_loss.item())\n",
    "            # ... (생략) ...\n",
    "\n",
    "    def visualize_training(self):\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(self.train_losses, label='Train Loss')\n",
    "        plt.plot(self.val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.show()\n",
    "\n",
    "class Evaluator:\n",
    "    # ... (생략) ...\n",
    "\n",
    "    def visualize_performance(self, y_pred):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        for i in range(3):\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            plt.scatter(self.y_test[:, i], y_pred[:, i], alpha=0.5)\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.xlabel(f'True Rubric {i+1} Score')\n",
    "            plt.ylabel(f'Predicted Rubric {i+1} Score')\n",
    "            plt.title(f'Rubric {i+1} Performance')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate(self):\n",
    "        # ... (생략) ...\n",
    "        y_pred = torch.cat((output1, output2, output3), dim=1).numpy()\n",
    "        # ... (생략) ...\n",
    "        self.visualize_performance(y_pred)\n",
    "        # ... (생략) ...\n",
    "\n",
    "class Predictor:\n",
    "    # ... (생략) ...\n",
    "\n",
    "def main():\n",
    "    file_path = 'scoring_data.tsv'\n",
    "\n",
    "    stop_words = ['은', '는', '이', '가', '을', '를', '에', '의', '과', '도', '으로', '만', '겠다', '습니다', '니다', '하다']\n",
    "    preprocessor = DataPreprocessor(stop_words)\n",
    "\n",
    "    synonym_dict = {'좋다': ['훌륭하다', '우수하다'], '크다': ['거대하다', '광대하다'], '작다': ['조그맣다', '왜소하다']}\n",
    "    insertion_words = ['그리고', '또한', '게다가']\n",
    "    deletion_prob = 0.2\n",
    "    augmenter = DataAugmenter(synonym_dict, insertion_words, deletion_prob)\n",
    "\n",
    "    dataset = ScoringDataset(file_path, preprocessor, augmenter)\n",
    "    dataset.preprocess()\n",
    "    dataset.augment()\n",
    "    dataset.tokenize()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = dataset.split_data()\n",
    "\n",
    "    hidden_dim = 64\n",
    "\n",
    "    model = ScoringModel(hidden_dim)\n",
    "\n",
    "    # 모델 구조 시각화\n",
    "    input_data = torch.randn(1, dataset.max_len).long()\n",
    "    output1, output2, output3, _, _, _ = model(input_data)\n",
    "    graph = make_dot(output1, params=dict(model.named_parameters()))\n",
    "    graph.render(\"model_graph\", format=\"png\")\n",
    "\n",
    "    trainer = Trainer(model, X_train, y_train, X_val, y_val, epochs=5, batch_size=16)\n",
    "    trainer.train()\n",
    "    trainer.visualize_training()\n",
    "\n",
    "    evaluator = Evaluator(model, X_test, y_test, dataset.tokenizer)\n",
    "    mse, mae, r2 = evaluator.evaluate()\n",
    "\n",
    "    predictor = Predictor(model, dataset.tokenizer)\n",
    "    prompts = ['새로운 주제글 1', '새로운 주제글 2']\n",
    "    exemplars = ['새로운 모범글 1', '새로운 모범글 2']\n",
    "    responses = ['새로운 응답글 1', '새로운 응답글 2']\n",
    "    y_pred = predictor.predict_scores(prompts, exemplars, responses)\n",
    "\n",
    "    print('\\n예측 결과:')\n",
    "    for i in range(len(prompts)):\n",
    "        print(f'주제글: {prompts[i]}')\n",
    "        print(f'모범글: {exemplars[i]}')\n",
    "        print(f'응답글: {responses[i]}')\n",
    "        print(f'예측 점수 - 루블릭1: {y_pred[i][0]:.2f}, 루블릭2: {y_pred[i][1]:.2f}, 루블릭3: {y_pred[i][2]:.2f}')\n",
    "        print('---')\n",
    "        \n",
    "    print('\\n모델 성능 정리:')\n",
    "    print(f'Test MSE: {mse:.4f}')\n",
    "    print(f'Test MAE: {mae:.4f}')\n",
    "    print(f'Test R^2: {r2:.4f}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n",
    "\n",
    "위 코드에서는 다음과 같은 리팩토링이 이루어졌습니다:\n",
    "\n",
    "1. `Trainer` 클래스에 `train_losses`와 `val_losses` 리스트를 추가하여 학습 과정에서의 손실 값을 기록합니다.\n",
    "2. `Trainer` 클래스에 `visualize_training` 메서드를 추가하여 학습 과정 동안의 손실 값을 선 그래프로 시각화합니다.\n",
    "3. `Evaluator` 클래스에 `visualize_performance` 메서드를 추가하여 모델의 예측값과 실제값을 산점도로 시각화합니다.\n",
    "4. `main` 함수에서 모델 구조 시각화를 위해 `torchviz` 라이브러리를 사용하여 계산 그래프를 생성하고 저장합니다.\n",
    "5. `main` 함수에서 `trainer.visualize_training()`을 호출하여 학습 과정 시각화를 수행합니다.\n",
    "\n",
    "이렇게 리팩토링된 코드는 모델 구조 시각화, 학습 과정 시각화, 모델 성능 시각화 모듈을 포함하고 있습니다. 각 모듈은 해당 기능을 수행하는 메서드를 클래스에 추가하여 구현되었습니다.\n",
    "\n",
    "코드를 실행하면 다음과 같은 결과를 얻을 수 있습니다:\n",
    "\n",
    "1. 모델 구조 시각화: \"model_graph.png\" 파일로 저장된 모델의 계산 그래프를 확인할 수 있습니다.\n",
    "2. 학습 과정 시각화: 학습 과정 동안의 훈련 손실과 검증 손실을 선 그래프로 시각화한 결과를 볼 수 있습니다.\n",
    "3. 모델 성능 시각화: 각 루브릭 점수에 대한 모델의 예측값과 실제값을 산점도로 시각화한 결과를 볼 수 있습니다.\n",
    "4. 예측 결과와 모델 성능 지표: 새로운 데이터에 대한 예측 결과와 모델의 성능 지표(MSE, MAE, R^2)를 출력합니다.\n",
    "\n",
    "이러한 시각화 모듈을 통해 모델의 구조, 학습 과정, 성능을 다양한 측면에서 분석하고 이해할 수 있습니다. 시각화 결과를 바탕으로 모델의 동작을 파악하고, 필요한 경우 모델의 개선 방향을 모색할 수 있습니다.\n",
    "\n",
    "코드의 가독성과 모듈화를 위해 각 기능을 담당하는 클래스와 메서드로 구조화하였으며, 주석을 통해 코드의 역할을 설명하였습니다.\n",
    "\n",
    "시각화 모듈은 모델 개발과 분석 과정에서 중요한 도구로 활용될 수 있으며, 모델의 성능 향상과 문제 해결에 도움을 줄 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
