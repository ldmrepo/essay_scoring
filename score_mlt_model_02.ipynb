{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torchviz import make_dot\n",
    "\n",
    "# 데이터 전처리\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # 불용어 제거\n",
    "        text = ' '.join([word for word in text.split() if word.lower() not in self.stop_words])\n",
    "        return text\n",
    "\n",
    "# 데이터셋\n",
    "class ScoringDataset:\n",
    "    def __init__(self, file_path, preprocessor):\n",
    "        self.data = pd.read_excel(file_path)\n",
    "        self.preprocessor = preprocessor\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.max_len = 512\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        # 데이터 전처리\n",
    "        self.data['text'] = self.data['주제글'] + ' ' + self.data['모범글'] + ' ' + self.data['수험자 응답글']\n",
    "        self.data['text'] = self.data['text'].apply(self.preprocessor.preprocess)\n",
    "        self.X = self.data['text'].tolist()\n",
    "        self.y = self.data[['루블릭1점수', '루블릭2점수', '루블릭3점수']].values\n",
    "\n",
    "    def tokenize_data(self):\n",
    "        # 데이터 토큰화\n",
    "        self.X = self.tokenizer.batch_encode_plus(\n",
    "            self.X,\n",
    "            max_length=self.max_len,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )['input_ids']\n",
    "\n",
    "    def split_data(self, test_size=0.2, val_size=0.2):\n",
    "        # 데이터 분할\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=test_size, random_state=42)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=42)\n",
    "        return X_train, X_val, X_test, torch.tensor(y_train, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# 어텐션 레이어\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(self.hidden_dim)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        context_vector = torch.matmul(attention_weights, value)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# 채점 모델\n",
    "class ScoringModel(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(ScoringModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.attention1 = AttentionLayer(self.bert.config.hidden_size)\n",
    "        self.attention2 = AttentionLayer(self.bert.config.hidden_size)\n",
    "        self.attention3 = AttentionLayer(self.bert.config.hidden_size)\n",
    "        self.hidden = nn.Linear(self.bert.config.hidden_size, hidden_dim)\n",
    "        self.output1 = nn.Linear(hidden_dim, 1)\n",
    "        self.output2 = nn.Linear(hidden_dim, 1)\n",
    "        self.output3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.size(1) > 512:\n",
    "            x = x[:, :512]  # 입력 데이터의 길이를 512로 조정\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)  # 배치 차원 추가\n",
    "        outputs = self.bert(x)\n",
    "        last_hidden_state = outputs[0]\n",
    "        context_vector1, attention_weights1 = self.attention1(last_hidden_state)\n",
    "        context_vector2, attention_weights2 = self.attention2(last_hidden_state)\n",
    "        context_vector3, attention_weights3 = self.attention3(last_hidden_state)\n",
    "        hidden1 = torch.relu(self.hidden(context_vector1[:, 0, :]))\n",
    "        hidden2 = torch.relu(self.hidden(context_vector2[:, 0, :]))\n",
    "        hidden3 = torch.relu(self.hidden(context_vector3[:, 0, :]))\n",
    "        output1 = self.output1(hidden1)\n",
    "        output2 = self.output2(hidden2)\n",
    "        output3 = self.output3(hidden3)\n",
    "        return output1, output2, output3, attention_weights1, attention_weights2, attention_weights3\n",
    "\n",
    "# 학습\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_data, val_data, epochs, batch_size, learning_rate):\n",
    "        self.model = model\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train(self):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            for i in range(0, len(self.train_data[0]), self.batch_size):\n",
    "                batch_X = self.train_data[0][i:i+self.batch_size].to(self.device)\n",
    "                batch_y = self.train_data[1][i:i+self.batch_size].to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output1, output2, output3, _, _, _ = self.model(batch_X)\n",
    "                loss = self.criterion(output1, batch_y[:, 0].unsqueeze(1)) + \\\n",
    "                       self.criterion(output2, batch_y[:, 1].unsqueeze(1)) + \\\n",
    "                       self.criterion(output3, batch_y[:, 2].unsqueeze(1))\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            train_loss /= len(self.train_data[0]) / self.batch_size\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_X = self.val_data[0].to(self.device)\n",
    "                val_y = self.val_data[1].to(self.device)\n",
    "                output1, output2, output3, _, _, _ = self.model(val_X)\n",
    "                val_loss = self.criterion(output1, val_y[:, 0].unsqueeze(1)) + \\\n",
    "                           self.criterion(output2, val_y[:, 1].unsqueeze(1)) + \\\n",
    "                           self.criterion(output3, val_y[:, 2].unsqueeze(1))\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{self.epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "        return train_losses, val_losses\n",
    "\n",
    "# 평가\n",
    "class Evaluator:\n",
    "    def __init__(self, model, test_data, tokenizer):\n",
    "        self.model = model\n",
    "        self.test_data = test_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_X = self.test_data[0].to(self.device)\n",
    "            test_y = self.test_data[1].numpy()\n",
    "            output1, output2, output3, attention_weights1, attention_weights2, attention_weights3 = self.model(test_X)\n",
    "            y_pred = torch.cat((output1, output2, output3), dim=1).cpu().numpy()\n",
    "\n",
    "        mse = mean_squared_error(test_y, y_pred)\n",
    "        mae = mean_absolute_error(test_y, y_pred)\n",
    "        r2 = r2_score(test_y, y_pred)\n",
    "\n",
    "        print(f\"Test MSE: {mse:.4f}, Test MAE: {mae:.4f}, Test R2: {r2:.4f}\")\n",
    "\n",
    "        # 어텐션 시각화\n",
    "        self.visualize_attention(test_X, attention_weights1, attention_weights2, attention_weights3)\n",
    "\n",
    "        # 성능 시각화\n",
    "        self.visualize_performance(test_y, y_pred)\n",
    "\n",
    "        return mse, mae, r2\n",
    "\n",
    "    def visualize_attention(self, test_X, attention_weights1, attention_weights2, attention_weights3):\n",
    "        sample_idx = 0\n",
    "        sample_input_ids = test_X[sample_idx].cpu().numpy()\n",
    "        sample_tokens = self.tokenizer.convert_ids_to_tokens(sample_input_ids)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "        for i, (ax, attention_weights) in enumerate(zip(axs, [attention_weights1, attention_weights2, attention_weights3])):\n",
    "            attention = attention_weights[sample_idx].cpu().numpy()\n",
    "            ax.imshow(attention, cmap='hot', interpolation='nearest', aspect='auto')\n",
    "            ax.set_xticks(range(len(sample_tokens)))\n",
    "            ax.set_xticklabels(sample_tokens, rotation=45, ha='right')\n",
    "            ax.set_yticks(range(attention.shape[0]))\n",
    "            ax.set_yticklabels([f'Layer {j+1}' for j in range(attention.shape[0])])\n",
    "            ax.set_title(f'Attention Weights - Rubric {i+1}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_performance(self, test_y, y_pred):\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "        for i, ax in enumerate(axs):\n",
    "            ax.scatter(test_y[:, i], y_pred[:, i], alpha=0.5)\n",
    "            ax.plot([0, 1], [0, 1], 'r--')\n",
    "            ax.set_xlabel('True Score')\n",
    "            ax.set_ylabel('Predicted Score')\n",
    "            ax.set_title(f'Rubric {i+1}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 예측\n",
    "class Predictor:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def predict(self, prompts, exemplars, responses):\n",
    "        input_texts = [f\"{prompt} {exemplar} {response}\" for prompt, exemplar, response in zip(prompts, exemplars, responses)]\n",
    "        input_ids = self.tokenizer.batch_encode_plus(\n",
    "            input_texts,\n",
    "            max_length=512,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )['input_ids'].to(self.device)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            output1, output2, output3, _, _, _ = self.model(input_ids)\n",
    "            scores = torch.cat((output1, output2, output3), dim=1).cpu().numpy()\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.78 GiB total capacity; 352.00 MiB already allocated; 3.06 MiB free; 352.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 58\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[23], line 25\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m     24\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2e-5\u001b[39m\n\u001b[0;32m---> 25\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, (X_train, y_train), (X_val, y_val), epochs, batch_size, learning_rate)\n\u001b[1;32m     26\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 학습 과정 시각화\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 114\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, train_data, val_data, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/auto-gpt/lib/python3.11/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/auto-gpt/lib/python3.11/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/auto-gpt/lib/python3.11/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/auto-gpt/lib/python3.11/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/auto-gpt/lib/python3.11/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/auto-gpt/lib/python3.11/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.78 GiB total capacity; 352.00 MiB already allocated; 3.06 MiB free; 352.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    # 데이터 로드 및 전처리\n",
    "    file_path = 'scoring_data.xlsx'\n",
    "    stop_words = ['은', '는', '이', '가', '을', '를', '에', '의', '과', '도', '으로', '만', '겠다', '습니다', '니다', '하다']\n",
    "    preprocessor = DataPreprocessor(stop_words)\n",
    "    dataset = ScoringDataset(file_path, preprocessor)\n",
    "    dataset.preprocess_data()\n",
    "    dataset.tokenize_data()\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = dataset.split_data()\n",
    "\n",
    "    # 모델 구성\n",
    "    hidden_dim = 64\n",
    "    model = ScoringModel(hidden_dim)\n",
    "\n",
    "    # 모델 구조 시각화\n",
    "    input_data = torch.randn(1, 512).long()\n",
    "    output1, output2, output3, _, _, _ = model(input_data)\n",
    "    graph = make_dot(output1, params=dict(model.named_parameters()))\n",
    "    graph.render(\"model_graph\", format=\"png\")\n",
    "\n",
    "    # 학습\n",
    "    epochs = 5\n",
    "    batch_size = 16\n",
    "    learning_rate = 2e-5\n",
    "    trainer = Trainer(model, (X_train, y_train), (X_val, y_val), epochs, batch_size, learning_rate)\n",
    "    train_losses, val_losses = trainer.train()\n",
    "\n",
    "    # 학습 과정 시각화\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "\n",
    "    # 평가\n",
    "    evaluator = Evaluator(model, (X_test, y_test), dataset.tokenizer)\n",
    "    mse, mae, r2 = evaluator.evaluate()\n",
    "\n",
    "    # 예측\n",
    "    prompts = ['새로운 주제글 1', '새로운 주제글 2']\n",
    "    exemplars = ['새로운 모범글 1', '새로운 모범글 2']\n",
    "    responses = ['새로운 응답글 1', '새로운 응답글 2']\n",
    "    predictor = Predictor(model, dataset.tokenizer)\n",
    "    scores = predictor.predict(prompts, exemplars, responses)\n",
    "\n",
    "    print(\"\\n예측 결과:\")\n",
    "    for i in range(len(prompts)):\n",
    "        print(f\"주제글: {prompts[i]}\")\n",
    "        print(f\"모범글: {exemplars[i]}\")\n",
    "        print(f\"응답글: {responses[i]}\")\n",
    "        print(f\"예측 점수: 루브릭1 - {scores[i][0]:.2f}, 루브릭2 - {scores[i][1]:.2f}, 루브릭3 - {scores[i][2]:.2f}\")\n",
    "        print()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
